{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFE 230P: ASSIGNMENT III\n",
    "**GROUP NAME:** [Group Name Here]\n",
    "\n",
    "Student ID | Name\n",
    ":--- | :---\n",
    "[Group Member 1 ID Here] | [Group Member 1 Name Here]\n",
    "[Group Member 2 ID Here] | [Group Member 2 Name Here]\n",
    "[Group Member 3 ID Here] | [Group Member 3 Name Here]\n",
    "[Group Member 4 ID Here] | [Group Member 4 Name Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature Engineering & Kernalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Feature Design\n",
    "\n",
    "Executing the cell below will create a pandas dataframe `data` containing 2015-16 daily adjusted percent returns for the top 99 US companies by market cap (as of 31 December 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>XOM</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>JNJ</th>\n",
       "      <th>JPM</th>\n",
       "      <th>GE</th>\n",
       "      <th>WFC</th>\n",
       "      <th>FB</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>...</th>\n",
       "      <th>MET</th>\n",
       "      <th>EOG</th>\n",
       "      <th>CL</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>PNC</th>\n",
       "      <th>EPD</th>\n",
       "      <th>VOO</th>\n",
       "      <th>ABT</th>\n",
       "      <th>SPG</th>\n",
       "      <th>NEE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20150102</th>\n",
       "      <td>-0.009513</td>\n",
       "      <td>0.006674</td>\n",
       "      <td>0.004110</td>\n",
       "      <td>-0.005897</td>\n",
       "      <td>-0.000478</td>\n",
       "      <td>-0.001438</td>\n",
       "      <td>-0.008310</td>\n",
       "      <td>-0.002189</td>\n",
       "      <td>0.005511</td>\n",
       "      <td>-0.003021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003328</td>\n",
       "      <td>0.001846</td>\n",
       "      <td>-0.002168</td>\n",
       "      <td>0.003990</td>\n",
       "      <td>-0.004494</td>\n",
       "      <td>0.019657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.002665</td>\n",
       "      <td>0.017737</td>\n",
       "      <td>0.003763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20150105</th>\n",
       "      <td>-0.028172</td>\n",
       "      <td>-0.009303</td>\n",
       "      <td>-0.027362</td>\n",
       "      <td>-0.020517</td>\n",
       "      <td>-0.006984</td>\n",
       "      <td>-0.031045</td>\n",
       "      <td>-0.018356</td>\n",
       "      <td>-0.027422</td>\n",
       "      <td>-0.016061</td>\n",
       "      <td>-0.020846</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033389</td>\n",
       "      <td>-0.060278</td>\n",
       "      <td>-0.007387</td>\n",
       "      <td>-0.016890</td>\n",
       "      <td>-0.030610</td>\n",
       "      <td>-0.065979</td>\n",
       "      <td>-0.017569</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.004101</td>\n",
       "      <td>-0.010591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20150106</th>\n",
       "      <td>0.000094</td>\n",
       "      <td>-0.014571</td>\n",
       "      <td>-0.005316</td>\n",
       "      <td>-0.022833</td>\n",
       "      <td>-0.004914</td>\n",
       "      <td>-0.025929</td>\n",
       "      <td>-0.021545</td>\n",
       "      <td>-0.020865</td>\n",
       "      <td>-0.013473</td>\n",
       "      <td>-0.023177</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030321</td>\n",
       "      <td>-0.028496</td>\n",
       "      <td>-0.010506</td>\n",
       "      <td>-0.030318</td>\n",
       "      <td>-0.025897</td>\n",
       "      <td>-0.023547</td>\n",
       "      <td>-0.009833</td>\n",
       "      <td>-0.011356</td>\n",
       "      <td>0.027834</td>\n",
       "      <td>0.003884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20150107</th>\n",
       "      <td>0.014022</td>\n",
       "      <td>0.012705</td>\n",
       "      <td>0.010133</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.022076</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.005951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013259</td>\n",
       "      <td>-0.006294</td>\n",
       "      <td>0.007963</td>\n",
       "      <td>-0.002866</td>\n",
       "      <td>0.011544</td>\n",
       "      <td>0.012801</td>\n",
       "      <td>0.012495</td>\n",
       "      <td>0.008108</td>\n",
       "      <td>0.018402</td>\n",
       "      <td>0.009154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20150108</th>\n",
       "      <td>0.038422</td>\n",
       "      <td>0.029418</td>\n",
       "      <td>0.016645</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.007863</td>\n",
       "      <td>0.022346</td>\n",
       "      <td>0.012043</td>\n",
       "      <td>0.022137</td>\n",
       "      <td>0.026592</td>\n",
       "      <td>0.003153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015234</td>\n",
       "      <td>0.038241</td>\n",
       "      <td>0.013606</td>\n",
       "      <td>0.037889</td>\n",
       "      <td>0.019251</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.017568</td>\n",
       "      <td>0.020554</td>\n",
       "      <td>-0.001437</td>\n",
       "      <td>0.006078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              AAPL      MSFT       XOM      AMZN       JNJ       JPM  \\\n",
       "date                                                                   \n",
       "20150102 -0.009513  0.006674  0.004110 -0.005897 -0.000478 -0.001438   \n",
       "20150105 -0.028172 -0.009303 -0.027362 -0.020517 -0.006984 -0.031045   \n",
       "20150106  0.000094 -0.014571 -0.005316 -0.022833 -0.004914 -0.025929   \n",
       "20150107  0.014022  0.012705  0.010133  0.010600  0.022076  0.001526   \n",
       "20150108  0.038422  0.029418  0.016645  0.006836  0.007863  0.022346   \n",
       "\n",
       "                GE       WFC        FB      GOOG    ...          MET  \\\n",
       "date                                                ...                \n",
       "20150102 -0.008310 -0.002189  0.005511 -0.003021    ...    -0.003328   \n",
       "20150105 -0.018356 -0.027422 -0.016061 -0.020846    ...    -0.033389   \n",
       "20150106 -0.021545 -0.020865 -0.013473 -0.023177    ...    -0.030321   \n",
       "20150107  0.000415  0.005951  0.000000 -0.001713    ...     0.013259   \n",
       "20150108  0.012043  0.022137  0.026592  0.003153    ...     0.015234   \n",
       "\n",
       "               EOG        CL      NVDA       PNC       EPD       VOO  \\\n",
       "date                                                                   \n",
       "20150102  0.001846 -0.002168  0.003990 -0.004494  0.019657  0.000000   \n",
       "20150105 -0.060278 -0.007387 -0.016890 -0.030610 -0.065979 -0.017569   \n",
       "20150106 -0.028496 -0.010506 -0.030318 -0.025897 -0.023547 -0.009833   \n",
       "20150107 -0.006294  0.007963 -0.002866  0.011544  0.012801  0.012495   \n",
       "20150108  0.038241  0.013606  0.037889  0.019251  0.012346  0.017568   \n",
       "\n",
       "               ABT       SPG       NEE  \n",
       "date                                    \n",
       "20150102 -0.002665  0.017737  0.003763  \n",
       "20150105  0.000223  0.004101 -0.010591  \n",
       "20150106 -0.011356  0.027834  0.003884  \n",
       "20150107  0.008108  0.018402  0.009154  \n",
       "20150108  0.020554 -0.001437  0.006078  \n",
       "\n",
       "[5 rows x 99 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\n",
    "    '../../data/top_99_returns.csv',\n",
    "    header=0,\n",
    "    index_col=0\n",
    ")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Engineer a few features (5-10) that, in your judgement, would be effective in characterizing the daily equity returns of each company. For example, $\\left|\\frac{\\max}{\\min}\\right|$ return or average squared returnâ€”be creative and use your instincts. Then, project the asset returns `data` into to this new feature space by creating a new matrix `data_fe` where each row represents a feature you engineered and each column represents a company. (Note that, unlike in previous assignments, the columns now represent observations while the rows represent features.)\n",
    "\n",
    "Center and scale each row of `data` and `data_fe` (you may design your own center and scaling protocol or stick with simple mean and standard deviation). Intuitively ration why we perform this standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Visualize Data\n",
    "\n",
    "Project data to 2D space using a method your choice so that the data can be visualized. Two recommended options include [Multi-Dimensional Scaling](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html) or [t-SNE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html).\n",
    "\n",
    "You do not need to understand what these lower-dimensional embedding algorithms do, just apply them and then plot the data in 2D. When displaying the points in 2D, plot the ticker of each company rather than a point so that the tickers are laid out on the 2D Cartesian plan. Do this for both `data` and `data_fe`.\n",
    "\n",
    "How does the raw data approach compare to your feature-mapping? Are the companies displayed in a way that respects your feature mapping? Briefly comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Kernelized Feature Space\n",
    "\n",
    "Create two square kernel matrices `kernel` and `kernel_fe` by applying a Gaussian kernel transformation to both the raw returns in `data` as well as their projection into your feature space, represented in `data_fe`. Then, use PCA to make a 10-dimensional approximation of the kernel matrix before continuing with MDS, t-SNE, or some other choice of low-dimensional embedding method to further reduce everything to 2D cooridnates. Then, plot the results.\n",
    "\n",
    "You will need to play with $\\gamma$ parameter defining the Gaussian kernel to get results that look interesting. Compare the two plots as you did in the previous part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Interpreting the Results\n",
    "\n",
    "In light of the previous analysis, what are the advantages and disadvantages of using engineered features in place of the raw data? What role does kernalization play? Briefly comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Least Squares and LAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. LAD Regression is a Linear Program\n",
    "\n",
    "Show that LAD regression can be written as a linear program. What does this mean computationally as compared to the equivalent problem in which $\\ell_1$ replaced with $\\ell_2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Adaptive Index Quantiles with Least Squares\n",
    "\n",
    "Executing the cell below will create a pandas dataframe `X` containing 2014-16 daily adjusted percent returns for the top 500 US companies by market cap (as of 31 December 2016) and another pandas dataframe `y` containing daily adjusted returns for the `SPY` ETF over the same period, which tracks the S&P 500 index closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv(\n",
    "    '../../data/top_500_returns.csv',\n",
    "    header=0,\n",
    "    index_col=0\n",
    ")\n",
    "\n",
    "y = pd.read_csv(\n",
    "    '../../data/spy.csv',\n",
    "    header=0,\n",
    "    index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tranform the data to log returns. Then, split the data into training and validation sets, where the first 70% of dates constitute the former.  Using only `cvxpy` and `numpy`, perform [least-norm regression](https://see.stanford.edu/materials/lsoeldsee263/08-min-norm.pdf) on the training set, regressing `SPY`'s return on the lagged returns of the top 500 companies:\n",
    "\n",
    "$$\\min_{\\theta} \\|\\theta\\|_2 \\text{ subject to } y_t = X_{t-1}\\theta$$\n",
    "\n",
    "Plot your model's performance on the validation set returns along with a historgram of your model's residuals on the validation set. Does the residual distribution appear to be Gaussian? Check your hypothesis using a statistical test such as [Shapiro-Wilks](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.stats.shapiro.html).\n",
    "\n",
    "One way to forecast the 1-step-ahead 95th percentile of index returns for `SPY` using lagged returns of the top 500 companies is to follow steps similar to those outlined in [this stack exchange post](https://stats.stackexchange.com/questions/147242/how-to-calculate-the-prediction-interval-for-an-ols-multiple-regression). Comment on the potential effectiveness of this method in light of the out-of-sample residual distribution you previously analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Adaptive Index Quantiles with LAD\n",
    "\n",
    "Perform quantile regression on the training set to estimate the 1-step-ahead 90th percentile of the `SPY`. Use the setting `solver=CBC` when calling the `.solve()` method in `cvxpy` **if you can** (this is not a requirement). How does this model perform on the validation set? Plot the daily returns of `SPY` with the forcasted $90^{th}$ percentile superimposed.\n",
    "\n",
    "Speculate about how this model would perform in practice compared to the least squares model in the previous part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Adaptive Index Quantiles with Ridged LAD\n",
    "\n",
    "Repeat the previous exercise, but with an $\\ell_2$ penalty on the vector of regression coefficients $\\theta$:\n",
    "\n",
    "$$\\theta_\\lambda = \\arg\\min_{\\theta} \\|y_t - X_{t-1}\\theta\\|_1 + \\lambda \\|\\theta\\|_2^2$$\n",
    "\n",
    "Tune the $\\ell_2$ penalty parameter $\\lambda$ using the validation set to obtain the optimal penalty $\\lambda^\\star$. (Make sure to think about what defines the optimal model.) How is the vector of regression coefficients $\\theta_{\\lambda^\\star}$ different from $\\theta_0$? How can you interpret this from an investing viewpoint? How does this model compare, on the validation set, to the model in the previous part? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Support Vector Machine\n",
    "\n",
    "In this question, we consider an application of SVM in text classification for volatility prediction. Executing the following cell will load a $1470 \\times 971$ `numpy` array into a variable `features` where each row corresponds to a published article and each column corresponds to the frequency of a keyword that appears in the article (i.e. our archive contains $1470$ articles and our dictionary contains $971$ keywords). Each article is about a certain company. Binary labels will also be imported as a $1470 \\times 1$  `numpy` array `labels`. An article's label is $+1$ if the article caused an immediate and significant change (positive or negative) to the company's stock price. Otherwise, the label is $-1$. The data has been divided into a training set (`Xtrain` and `ytrain`), which will be used to train you SVM, and a validation set (`Xtest` and `ytest`) which will be used to test the SVM's prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.loadtxt('../../data/news/features.csv', delimiter=',') # load data here\n",
    "labels = np.loadtxt('../../data/news/labels.csv', delimiter=',') # load data here\n",
    "\n",
    "Xtrain = features[:-106]\n",
    "ytrain = labels[:-106]\n",
    "\n",
    "Xtest = features[-106:]\n",
    "ytest = labels[-106:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. $\\ell_2$ SVM\n",
    "\n",
    "The ridged-SVM classification problem can be formulated as the following optimization problem:\n",
    "\n",
    "$$\\underset{w, b}{\\text{min }} \\frac{1}{2}\\left\\|w\\right\\|_2^2 + C\\sum_{i=1}^{1470}{\\left(1 - y_i\\left(w^\\top x_i + b\\right)\\right)_+}$$\n",
    "\n",
    "where $y_i$ denotes the $i^{th}$ label, $x_i$ denotes the $i^{th}$ vector of word frequencies in the articles, $w$ is the weights or vector of coefficients, $b$ is the offset or intercept, and $C$ is a model parameter is inversely related to the ridge regularization of the weights vector $w$. This is a quadratic optimization problem.\n",
    "\n",
    "Using `cvxpy`, implement this SVM (estimate the $w$ and $b$ parameters) on the training set and tune the parameter $C$ from $0$ to $100$ by checking classification accuracy on the validation set. Plot the training accuracy versus $C$ curve and validation accuracy versus $C$ curve. Briefly comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Feature Selection by Magnitude\n",
    "\n",
    "We want to find the keywords that are most important for classification. Consider the following approach: Fix $C=10$ and perform SVM to estimate $w$ on the training set. Sort elements of $w$ by their absolute value in descending order, choose a the top $k$, and then perform SVM on this subset of the features. What are some advantages or disadvantages you anticipate in approaching feature selection in this manner?\n",
    "\n",
    "Try $k\\in \\{10, 20, 30, \\dots, 190, 200\\}$ and evaluate classification accuracy on the validation set. Comment on the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. $\\ell_1$ SVM\n",
    "\n",
    "Consider a different approach for feature selection: Subsitute the $\\ell_2$-norm penalty with an $\\ell_1$-norm penalty in the SVM objective:\n",
    "\n",
    "$$\\underset{w, b}{\\text{min }} \\left\\|w\\right\\|_1 + C\\sum_{i=1}^{1470}{\\left(1 - y_i\\left(w^\\top x_i + b\\right)\\right)_+}$$\n",
    "\n",
    "How might this approach compare to the previous feature-selection approach suggested in part B above?\n",
    "\n",
    "Sweep the parameter $C$ in the interval $[0, 100]$ and plot the number of non-zero elements in $w$ versus the prameter $C$. Note that due to `cvxpy`'s limited numerical precision, zero elements are not exactly $0$. So the criterion for non-zero element is $|w_i| > 10^{-6}$. Perform a similar plot of validation set accuracy versus the parameter $C$. Comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
