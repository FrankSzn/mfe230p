{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFE 230P: ASSIGNMENT III\n",
    "**YOUR STUDENT ID** \\\\ YOUR NAME \\\\ YOUR GROUP NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature Engineering & Spectral $k$-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Similarity from Distance\n",
    "\n",
    "Show that if $d(\\cdot, \\cdot)$ is a [distance metric](http://bit.ly/2qMIu7d), then\n",
    "\n",
    "$$\\kappa_\\gamma(x, y) = \\exp\\left\\{-\\gamma \\cdot d(x, y)\\right\\}$$\n",
    "\n",
    "is a [positive-definite kernel](https://en.wikipedia.org/wiki/Positive-definite_kernel) for all $\\gamma > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Feature Design\n",
    "\n",
    "Executing the cell below will create a pandas dataframe `data` containing 2015-16 daily adjusted percent returns for the top 99 US companies by market cap (as of 31 December 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\n",
    "    '../../data/top_99_returns.csv',\n",
    "    header=0,\n",
    "    index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Engineer a few features (5-10) that, in your judgement, would be effective in characterizing the daily equity returns of each company. For example, (maximum/minimum) or average squared return—be creative and use your instincts. Then, project the asset returns `data` into to this new feature space by creating a new matrix `data_fe` where each row represents a company and each column represents a feature you engineered.\n",
    "\n",
    "Normalize each column of `data_fe` to have zero mean and unit standard deviation. Intuitively ration why we perform this standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Kernel Matrix\n",
    "\n",
    "Create a square kernel matrix `kernel` and `kernel_fe` by applying the kernel transformation discussed in part A to both the raw returns in `data` as well as their projection into your feature space, represented in `data_fe`. You may set $\\gamma = 1$ and use the distance metric $d(x,y) = \\|x - y\\|_2^2$ or choose your own distance metric (just make sure it is a distance metric)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Spectral Clustering\n",
    "\n",
    "Perform 3-means clustering on the top 3 princicpal components of both `kernel` and `kernel_fe`. Print the tickers associated with each of the three clusters for both `kernel` and `kernel_fe`.\n",
    "\n",
    "Which method provides more intuitive groupings (this is a subjective question)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Interpreting the Results\n",
    "\n",
    "In light of the previous analysis, what are the advantages and disadvantages of useing engineered features in place of the raw data? How would the analysis be different if we had implemented $k$-means on the raw returns (without a kernel transform)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Least Squares and LAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. LAD Regression is a Linear Program\n",
    "\n",
    "Show that LAD regression can be written as a linear program in standard form. What does this mean computationally as compared to the equivalent problem in which $\\ell_1$ replaced with $\\ell_2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Adaptive Index Quantiles with Least Squares\n",
    "\n",
    "Executing the cell below will create a pandas dataframe `X` containing 2015-16 daily adjusted percent returns for the top 500 US companies by market cap (as of 31 December 2016) and another pandas dataframe `y` containing daily adjusted returns for the `SPY` ETF over the same period, which tracks the S&P 500 index closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv(\n",
    "    '../../data/top_750_returns.csv',\n",
    "    header=0,\n",
    "    index_col=0\n",
    ")\n",
    "\n",
    "y = pd.read_csv(\n",
    "    '../../data/spy.csv',\n",
    "    header=0,\n",
    "    index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and validation sets, where the first 75% or so of dates constitute the former. \n",
    "\n",
    "Using only `cvxpy` and `numpy`, perform [least-norm regression](https://see.stanford.edu/materials/lsoeldsee263/08-min-norm.pdf), regressing `SPY`'s return on the lagged returns of the top 750 companies. \n",
    "\n",
    "$$\\min_{\\theta} \\|\\theta\\|_2^2 \\text{ subject to } y_t = X_{t-1}\\theta$$\n",
    "\n",
    "Use this model to forecast 1-step-ahead 95th percentile of index returns on the validation set. Follow the steps in [this stack exchange post](https://stats.stackexchange.com/questions/147242/how-to-calculate-the-prediction-interval-for-an-ols-multiple-regression) except replace $(X^\\top X)^{-1}$ with the [pseudo-inverse](https://en.wikipedia.org/wiki/Moore–Penrose_pseudoinverse) $X^+$. \n",
    "\n",
    "How accurate is your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Adaptive Index Quantiles with LAD\n",
    "\n",
    "Perform quantile regression on the training set to estimate the 1-step-ahead 95th percentile of the `SPY`. Use the setting `solver=CBC` when calling the `.solve()` method in `cvxpy`. How does this model compare, on the validation set, to the least squares model in the previous part? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Adaptive Index Quantiles with Ridged LAD\n",
    "\n",
    "Repeat the previous exercise, but with a ridge penalty on the vector of regression coefficients $\\theta$. Tune the ridge penalty parameter $\\lambda$ using the validation set. How is the vector of regression coefficients $\\theta_\\lambda$ different than in the previous part? How can you interpret this from an investing viewpoint? How does this model compare, on the validation set, to the least squares model in the previous part? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Support Vector Machine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
