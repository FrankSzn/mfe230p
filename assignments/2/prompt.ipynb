{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFE 230P: ASSIGNMENT II\n",
    "`Due 21 June 2017 at or before 23:59:59`\n",
    "\n",
    "**NAME:**\n",
    "\n",
    "**STUDENT ID:**\n",
    "\n",
    "**COLLABORATORS:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Low-Rank Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. PCA and Convexity\n",
    "\n",
    "**True or False:** _The pca problem_\n",
    "\n",
    "$$\\min_{Z} \\|A - Z\\|^2_F \\text{ subject to } \\mathbf{rank}(Z) \\leq k$$\n",
    "\n",
    "\n",
    "\n",
    "_is a convex optimization problem, where $0 < k < \\min\\{m,n\\}$._ Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Nuclear Norm Denoising\n",
    "\n",
    "Recall that, for any symetric positive definite matrix $A \\in \\mathbb{R}^{n\\times n}$,\n",
    "\n",
    "$$P \\begin{bmatrix}\n",
    "\\mathbb{I}_k & \\mathbf{0} \\\\\n",
    "\\mathbf{0} & \\mathbf{0}\n",
    "\\end{bmatrix}\n",
    "D P^\\top = \\arg\\min_{Z} \\frac{1}{2}\\|A - Z\\|^2_F \\text{ subject to } \\mathbf{rank}(Z) \\leq k$$\n",
    "\n",
    "where $0 < k < n$, $A = PD P^\\top$ via EVD, and $\\mathbb{I}_k$ is the $k\\times k$ identity matrix. Practitioners (such as those at Netflix) often work with a close, \"relaxed\" version of the above problem known as **Nuclear Norm Denoising**:\n",
    "\n",
    "$$\\min_{Z} \\frac{1}{2} \\|A - Z\\|^2_F + \\lambda\\|Z\\|_*$$\n",
    "\n",
    "in which $\\lambda > 0$ serves a similar purpose to $k$ in the former problem. \n",
    "\n",
    "Express the solution to the nuclear norm denoising problem in closed form. **Hint:** Both the Forbenius norm $\\|\\cdot\\|_F$ and the nuclear norm $\\|\\cdot\\|_*$ are invariant to orthogonal rotation. Furthermore, for $\\lambda >0$,\n",
    "\n",
    "$$(y - \\lambda)_+ = \\arg\\min_x \\frac{(y - x)^2}{2} + \\lambda |x|$$\n",
    "\n",
    "where $x$ and $y$ are scalars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Robust PCA and Convexity\n",
    "\n",
    "**True or False:** _The robust pca problem_\n",
    "\n",
    "$$\\min_{Z, S} \\|Z\\|_* + \\lambda\\|S\\|_1 \\text{ subject to } A = Z + S$$\n",
    "\n",
    "_is a convex optimization problem, where $\\lambda > 0$._ Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Robust PCA as a Generalized Low-Rank Model\n",
    "\n",
    "Recall from lecture that a **generalized low-rank model** is defined by the problem\n",
    "\n",
    "$$\\min_{X,Y} \\mathcal{L}\\left(X, Y\\right) + \\ell_x\\left(X\\right) + \\ell_y\\left(Y\\right)$$\n",
    "\n",
    "where $\\mathcal{L}$ is bi-convex in $(X, Y)$ and $\\ell$ is convex. Show that robust PCA can be formulated as a generalized low-rank model. _**Hint:** For any matrix $Z = XY^\\top$, $\\|Z\\|_* = \\frac{1}{2}\\left( \\|X\\|^2_F + \\|Y\\|^2_F\\right)$._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION.\n",
    "\n",
    "_Your solution here._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
